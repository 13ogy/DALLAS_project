#!/usr/bin/env python3
"""
Render a comprehensive LaTeX report (paper.tex) ~15–20 pages, integrating:
- All key parts from cline/project.md and cline/course.md mapped to this project
- Detailed methods, results, interpretation, risks/ethics, and appendices
- Figures/tables generated by the pipeline

Inputs (generated earlier by scripts):
- outputs/metrics/metrics.json
- outputs/tables/elasticity_summary.json
- outputs/tables/data_dictionary.json
- outputs/tables/feature_importance_permutation_val.csv
- outputs/tables/per_building_mae_test.csv
- outputs/tables/preds_sample_test.csv
- outputs/vulnerability/building_elasticity.csv
- outputs/figures/*.png

Outputs:
- report/paper.tex
- report/README.md
"""

from pathlib import Path
import json
import pandas as pd
from textwrap import dedent

ROOT = Path(".")
REPORT_DIR = ROOT / "report"
FIG_DIR = ROOT / "outputs" / "figures"
TAB_DIR = ROOT / "outputs" / "tables"
METRICS = ROOT / "outputs" / "metrics" / "metrics.json"
ELAST = TAB_DIR / "elasticity_summary.json"
DICT = TAB_DIR / "data_dictionary.json"
FI_CSV = TAB_DIR / "feature_importance_permutation_val.csv"
PER_BLD_CSV = TAB_DIR / "per_building_mae_test.csv"
VULN_CSV = ROOT / "outputs" / "vulnerability" / "building_elasticity.csv"

def load_json_safe(path, default):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return default

def fmt(v, nd=4):
    try:
        return f"{float(v):.{nd}f}"
    except Exception:
        return "NA"

def pct_improve(baseline, model):
    try:
        if baseline is None or model is None:
            return "NA"
        return f"{(1.0 - float(model) / float(baseline)) * 100.0:.1f}\\%"
    except Exception:
        return "NA"

def latex_escape(s: str) -> str:
    r"""
    Escape common LaTeX special characters in text content.
    Minimal set needed for tables: \\, \%, \_, \&, \#, \{, \}, \$
    """
    if s is None:
        return ""
    s = str(s)
    # Order matters: escape backslash first
    s = s.replace("\\", "\\textbackslash{}")
    s = s.replace("&", "\\&")
    s = s.replace("%", "\\%")
    s = s.replace("_", "\\_")
    s = s.replace("#", "\\#")
    s = s.replace("{", "\\{")
    s = s.replace("}", "\\}")
    s = s.replace("$", "\\$")
    return s

def read_per_building_summary():
    if not PER_BLD_CSV.exists():
        return None
    try:
        df = pd.read_csv(PER_BLD_CSV)
        if "MAE" not in df.columns:
            return None
        s = df["MAE"].dropna()
        if s.empty:
            return None
        summary = {
            "count": int(s.shape[0]),
            "min": float(s.min()),
            "q25": float(s.quantile(0.25)),
            "median": float(s.median()),
            "q75": float(s.quantile(0.75)),
            "max": float(s.max())
        }
        return summary
    except Exception:
        return None

def read_top_features(n=12):
    if not FI_CSV.exists():
        return []
    try:
        df = pd.read_csv(FI_CSV)
        df = df.sort_values("importance_mean", ascending=False)
        rows = df.head(n).to_dict(orient="records")
        return rows
    except Exception:
        return []

def read_vulnerability_tables(n=12):
    if not VULN_CSV.exists():
        return [], []
    try:
        df = pd.read_csv(VULN_CSV)
        if "building_name" not in df.columns or "elasticity" not in df.columns:
            return [], []
        dfn = df.dropna(subset=["elasticity"]).copy()
        if dfn.empty:
            return [], []
        high = dfn.sort_values("elasticity", ascending=False).head(n)
        low = dfn.sort_values("elasticity", ascending=True).head(n)
        high_rows = high[["building_name", "elasticity"]].to_dict(orient="records")
        low_rows = low[["building_name", "elasticity"]].to_dict(orient="records")
        return high_rows, low_rows
    except Exception:
        return [], []

def main():
    REPORT_DIR.mkdir(parents=True, exist_ok=True)

    metrics = load_json_safe(METRICS, {})
    elast = load_json_safe(ELAST, {})
    dd = load_json_safe(DICT, {})

    # Extract metrics
    naive_val_mae = metrics.get("Naive", {}).get("val", {}).get("MAE", None)
    naive_val_rmse = metrics.get("Naive", {}).get("val", {}).get("RMSE", None)
    naive_test_mae = metrics.get("Naive", {}).get("test", {}).get("MAE", None)
    naive_test_rmse = metrics.get("Naive", {}).get("test", {}).get("RMSE", None)

    rf_val_mae = metrics.get("RF", {}).get("val", {}).get("MAE", None)
    rf_val_rmse = metrics.get("RF", {}).get("val", {}).get("RMSE", None)
    rf_test_mae = metrics.get("RF", {}).get("test", {}).get("MAE", None)
    rf_test_rmse = metrics.get("RF", {}).get("test", {}).get("RMSE", None)

    hg_val_mae = metrics.get("HGBR", {}).get("val", {}).get("MAE", None)
    hg_val_rmse = metrics.get("HGBR", {}).get("val", {}).get("RMSE", None)
    hg_test_mae = metrics.get("HGBR", {}).get("test", {}).get("MAE", None)
    hg_test_rmse = metrics.get("HGBR", {}).get("test", {}).get("RMSE", None)

    t_train_end = metrics.get("cutoffs", {}).get("train_end", "NA")
    t_val_end = metrics.get("cutoffs", {}).get("val_end", "NA")

    rows = dd.get("stats", {}).get("rows", None)
    n_bld = dd.get("stats", {}).get("unique_buildings", None)
    time_start = dd.get("stats", {}).get("time_start", "NA")
    time_end = dd.get("stats", {}).get("time_end", "NA")

    seg_counts = elast.get("segments_count", {})
    seg_low = seg_counts.get("Low", 0)
    seg_mod = seg_counts.get("Moderate", 0)
    seg_high = seg_counts.get("High", 0)
    med_el = elast.get("median_elasticity", None)
    q25_el = elast.get("q25_elasticity", None)
    q75_el = elast.get("q75_elasticity", None)
    n_with_scores = elast.get("n_with_scores", 0)
    n_buildings_total = elast.get("n_buildings", 0)

    per_bld = read_per_building_summary()
    top_feats = read_top_features(14)
    top_high, top_low = read_vulnerability_tables(12)
    top_feat_names = ", ".join([latex_escape(r.get("feature","")) for r in top_feats[:5]]) if top_feats else ""

    # Figures (relative from report/)
    fig_paths = {
        "fi": "../outputs/figures/feature_importance_bar.png",
        "scatter": "../outputs/figures/pred_vs_actual_scatter.png",
        "heatmap": "../outputs/figures/error_heatmap_hour_month.png",
        "resid_hour": "../outputs/figures/residual_by_hour.png",
        "series": "../outputs/figures/pred_vs_actual_timeseries.png",
        "usage_temp": "../outputs/figures/usage_vs_temp_scatter.png",
        "elasticity": "../outputs/figures/elasticity_distribution.png",
    }

    # Build LaTeX rows
    top_feat_rows = ""
    if top_feats:
        for r in top_feats:
            name = latex_escape(r.get("feature", ""))
            imp = r.get("importance_mean", None)
            std = r.get("importance_std", None)
            top_feat_rows += f"{name} & {fmt(imp,3)} & {fmt(std,3)} \\\\\n"
    per_bld_summary_block = "NA"
    if per_bld:
        per_bld_summary_block = (
            f"n={per_bld['count']}, min={fmt(per_bld['min'])}, "
            f"Q1={fmt(per_bld['q25'])}, median={fmt(per_bld['median'])}, "
            f"Q3={fmt(per_bld['q75'])}, max={fmt(per_bld['max'])}"
        )
    high_rows = ""
    for r in top_high:
        high_rows += f"{latex_escape(r['building_name'])} & {fmt(r['elasticity'],4)} \\\\\n"
    low_rows = ""
    for r in top_low:
        low_rows += f"{latex_escape(r['building_name'])} & {fmt(r['elasticity'],4)} \\\\\n"

    # LaTeX fallback rows (avoid backslashes inside f-string expression parts)
    no_table_triple = " \\multicolumn{3}{c}{(Table not available)} \\\\"
    no_table_double = " \\multicolumn{2}{c}{(Not available)} \\\\"

    # Improvements
    imp_mae_val_vs_naive = pct_improve(naive_val_mae, hg_val_mae)
    imp_mae_test_vs_naive = pct_improve(naive_test_mae, hg_test_mae)
    imp_rmse_val_vs_naive = pct_improve(naive_val_rmse, hg_val_rmse)
    imp_rmse_test_vs_naive = pct_improve(naive_test_rmse, hg_test_rmse)

    data_overview_rows = [
        ("Rows", str(rows) if rows is not None else "NA"),
        ("Buildings", str(n_bld) if n_bld is not None else "NA"),
        ("Time range", f"{time_start} to {time_end}"),
        ("Train end", str(t_train_end)),
        ("Validation end", str(t_val_end)),
        ("Test period", f"$>$ {t_val_end}"),
    ]
    feature_catalog_rows = [
        ("Calendar", "hour, day_of_week, month, season, is_weekend, is_holiday"),
        ("Dynamics", "lag_1h, lag_24h, rollmean_24h (leakage-safe)"),
        ("Weather", "apparent_temperature_norm, precipitation, is_day"),
        ("Target", "y_next (next-hour usage per building)"),
    ]
    feat_cat_latex = "\n".join([f"{k} & {latex_escape(v)} \\\\" for k, v in feature_catalog_rows])
    data_over_latex = "\n".join([f"{k} & {v} \\\\" for k, v in data_overview_rows])

    # Model configs (from training script)
    rf_config = r"""
    \begin{{itemize}}[leftmargin=*]
      \item n\_estimators=200, max\_depth=12, min\_samples\_leaf=5
      \item max\_features="sqrt", n\_jobs=-1, random\_state=42
    \end{{itemize}}
    """
    hgbr_config = r"""
    \begin{{itemize}}[leftmargin=*]
      \item loss="squared\_error", learning\_rate=0.05, max\_depth=8
      \item max\_iter=300, l2\_regularization=0.0, random\_state=42
    \end{{itemize}}
    """

    # Long background sections (Course + Project mapping)
    background_sections = r"""
    \section{Methods and Implementation}
    This section describes the end-to-end methodology used in this project and motivates key design choices tied to the results, forming one coherent narrative for forecasting and vulnerability profiling.

    \subsection{Data Acquisition, Web Services, and Scraping}
    We operate on provided CSVs; nonetheless, real deployments often interface with RESTful APIs for weather and meter data.
    \begin{{itemize}}[leftmargin=*]
      \item \textbf{HTTP/REST}: Use GET/POST with authentication, pagination, retries, and backoff for robustness.
      \item \textbf{JSON ingestion}: Validate schemas and status codes; enforce idempotency in write flows.
      \item \textbf{Scraping fallback}: For non-API sources, responsibly use headless browsers; obey robots.txt and ToS.
    \end{{itemize}}

    \subsection{GDPR and Ethics}
    Our pipelines respect privacy by using anonymized identifiers and avoiding geolocation re-identification.
    \begin{itemize}[leftmargin=*]
      \item \textbf{Principles}: data minimization, purpose limitation, transparency, security, and accountability.
      \item \textbf{Rights}: access, rectification, erasure, portability, and objection.
    \end{itemize}

    \subsection{Exploratory Data Analysis (EDA) and Wrangling}
    We examine missingness, outliers, and time-zone consistency before modeling.
    \begin{itemize}[leftmargin=*]
      \item \textbf{Checklist}: shape, dtypes, summary stats, \texttt{head()/sample()}, \texttt{nunique()}, missingness heatmaps.
      \item \textbf{Time columns}: ensure timezone handling and DST; align to hourly granularity.
    \end{itemize}

    \subsection{Missing Data Strategies}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Short gaps}: linear interpolation; \textbf{structural gaps}: avoid fabrication; retain informative zeros when justified.
      \item We drop hours lacking weather to prevent biased exogenous imputations, preserving sensitivity estimation integrity.
    \end{itemize}

    \subsection{Transformations and Encoding}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Per-building MinMax} for usage to eliminate magnitude bias and focus on shape.
      \item One-hot or integer encoding for calendar; trees handle ordinal/categorical safely without global scaling.
    \end{itemize}

    \subsection{Visualization Best Practices}
    \begin{itemize}[leftmargin=*]
      \item Use histograms/KDE for distribution, scatter for relationships, and heatmaps for hourly patterns.
      \item Avoid dual y-axes unless necessary and clearly annotated; always label units and baselines.
    \end{itemize}

    \subsection{Dimensionality Reduction (PCA) and Nonlinear Projections}
    We primarily use trees on engineered features; PCA/t-SNE are optional for EDA to visualize clusters or regime shifts. PCA projects centered/scaled features into components explaining variance; t-SNE reveals local structure but distorts global distances.

    \subsection{ANOVA and ANCOVA}
    For hypothesis-driven analysis (e.g., comparing groups of buildings), ANOVA tests mean differences; ANCOVA adjusts for covariates such as temperature. In this work, we prioritize predictive modeling; inferential tests may augment future analyses.

    \subsection{Bias in Machine Learning and Mitigation}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Sampling bias}: station density or urban concentration can skew elasticity—monitor subgroup performance.
      \item \textbf{Measurement bias}: sensor outages differ by region; quantify missingness patterns.
      \item \textbf{Mitigation}: enrich underrepresented segments, reweight, and prefer explainable models.
    \end{itemize}

    \subsection{ML Protocol: Tasks, Splits, Losses, and Regularization}
    We treat energy forecasting as regression, with MAE and RMSE as primary metrics.
    \begin{itemize}[leftmargin=*]
      \item \textbf{Losses}: MAE for robustness; RMSE to penalize large errors.
      \item \textbf{Regularization}: tree depth constraints, subsampling, and early stopping (for boosted libraries).
      \item \textbf{Reproducibility}: fixed seeds, version pinning, and manifest of features/hyperparameters.
    \end{itemize}

    \subsection{Algorithms and Evaluation}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Random Forest}: bagging of trees with random feature subsets; strong and stable baseline.
      \item \textbf{Gradient Boosting}: sequential residual fitting; powerful on tabular data and interpretable.
      \item \textbf{Design}: chronological CV or rolling windows; report fold variability and confidence intervals if applicable.
    \end{itemize}

    \subsection{Applying to Energy Forecasting}
    \begin{enumerate}[leftmargin=*]
      \item Ingest/unify; align to hourly; ensure weather availability per row.
      \item Engineer leakage-safe features (lags/rolling) per building.
      \item Chronological splits (80/10/10); tune with time-aware folds.
      \item Train boosted trees; compare against Naive and RF.
      \item Explain with permutation importance and ICE/PDP for temperature.
      \item Segment by elasticity; quantify uncertainty and ethical risks.
    \end{enumerate}

    \subsection{Pitfalls and How We Avoided Them}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Entity leakage}: all lags/rolling computed per building with explicit \texttt{groupby} and \texttt{shift()}.
      \item \textbf{Random splits}: avoided; splits are chronological.
      \item \textbf{Global scalers}: not used for leakage-prone steps; tree models are scale-robust.
    \end{itemize}
    """

    project_mapping = r"""
    \section{Project Overview Mapped to Implementation}
    \subsection{Problem Statement and Objectives}
    Predict next-hour consumption and profile temperature sensitivity per building. Deliver accurate forecasts (MAE/RMSE) and actionable elasticity segments (High/Moderate/Low).

    \subsection{Data Sources, Structure, and Dimensions}
    Long-format hourly data: one row per building-hour with weather covariates. We processed \textbf{~%s} rows over \textbf{%s} buildings from \textbf{%s} to \textbf{%s}.
    """ % (str(rows) if rows is not None else "NA", str(n_bld) if n_bld is not None else "NA", str(time_start), str(time_end))

    project_mapping += r"""
    \subsection{Preprocessing and Alignment}
    Timestamps floored to hour; rows lacking weather dropped. Holidays and seasons derived (AU mapping). Usage standardized to kWh upstream; we re-normalized per building.

    \subsection{Feature Engineering}
    \begin{itemize}[leftmargin=*]
      \item Calendar: hour, day\_of\_week, month, season, weekend, holiday.
      \item Dynamics: lag\_1h (persistence), lag\_24h (daily cycle), rollmean\_24h (recent regime).
      \item Weather: apparent\_temperature\_norm (driver), precipitation (behavioral proxy), is\_day.
    \end{itemize}

    \subsection{Target Definition}
    Next-hour usage (\texttt{y\_next}) via per-building \texttt{shift(-1)}. Final training rows require non-missing lags/rolling and target.

    \subsection{Splitting Strategy and Evaluation}
    Chronological splits using timestamp quantiles: Train $\le$ %s; Val $\le$ %s; Test $>$ Val end. Metrics: MAE (primary), RMSE (secondary).
    """ % (str(t_train_end), str(t_val_end))

    project_mapping += r"""
    \subsection{Models}
    \begin{itemize}[leftmargin=*]
      \item \textbf{Naive}: $\hat{y}(t)=y(t-1)$
      \item \textbf{Random Forest}: stability baseline; moderate depth.
      \item \textbf{HistGradientBoosting}: primary model; fast and strong on tabular data.
    \end{itemize}

    \subsection{Outputs and Explainability}
    Forecast tables and error diagnostics; global permutation importance; temperature elasticity via ICE slopes to segment buildings.

    \subsection{Risks, Ethics, and Bias}
    Station coverage and regional composition can bias elasticity distributions; subgroup monitoring recommended. Privacy preserved via anonymized IDs; report residual uncertainty and avoid overclaiming precision.

    \subsection{Mini-Glossary}
    \textbf{Leakage}: future or cross-entity information contaminating training. 
    \textbf{Elasticity}: $\frac{d\hat{y}}{d\,temp}$—sensitivity of predictions to temperature.
    \textbf{Exogenous}: driver not generated by the system (e.g., weather).
    """

    # Compose the LaTeX document
    paper = dedent(rf"""
    \documentclass[11pt,a4paper]{{article}}
    \usepackage[margin=1in]{{geometry}}
    \usepackage{{graphicx}}
    \usepackage{{booktabs}}
    \usepackage{{siunitx}}
    \usepackage[T1]{{fontenc}}
    \usepackage[utf8]{{inputenc}}
    \usepackage{{textcomp}}
    \usepackage{{microtype}}
    \usepackage{{hyperref}}
    \usepackage{{float}}
    \usepackage{{caption}}
    \usepackage{{xcolor}}
    \usepackage{{longtable}}
    \usepackage{{array}}
    \usepackage{{enumitem}}
    \usepackage{{tikz}}
    \usetikzlibrary{{arrows.meta,positioning}}

    \title{{Smart Energy Analytics -- Hourly Forecasting and Weather-Vulnerability Profiling}}
    \author{{DALLAS Project}}
    \date{{\today}}

    \begin{{document}}
    \maketitle

    \begin{{abstract}}
    We develop a reproducible pipeline to forecast next-hour electricity consumption and quantify temperature-driven vulnerability at the building level. The pipeline builds a weather-aligned hourly dataset, creates leakage-safe time features, trains and validates tree-based models with time-aware splits, and computes temperature elasticity to segment buildings by weather sensitivity. The final artifacts include trained models, metrics, explainability plots, and a vulnerability report.
    \end{{abstract}}

    \tableofcontents
    \clearpage

    \section{{Introduction}}
    Data-driven prediction has become a core capability across industries: from demand forecasting in energy and retail, to risk scoring in finance, to real-time decision support in operations. Over the last decade, modern machine learning for tabular and time-series data, better data engineering practices, and MLOps have collectively improved accuracy and reliability. Yet, in production environments, three constraints dominate: (i) leakage-free data design and time-aware evaluation, (ii) interpretability and diagnostics that stakeholders can trust, and (iii) simple, automatable paths to deployment.
    
    This project contributes along those lines in the energy domain. We assemble a weather-aligned, hourly dataset spanning multiple regions, engineer leakage-safe temporal and exogenous features, and train competitive tree-based models with chronological splits that reflect deployment. We interpret predictions at both global and per-building levels, quantify temperature sensitivity as a vulnerability signal, and package the workflow into reproducible scripts and a lightweight CLI for batch prediction. The aim is not only to achieve strong accuracy, but to demonstrate a transparent, end-to-end forecasting pipeline that can be maintained and extended in real settings.


    \section{{Pipeline Overview}}
    Our pipeline spans data discovery to deployment to reflect how forecasting systems are built and operated in practice:
    \begin{{enumerate}}[leftmargin=*]
      \item Source discovery and acquisition (APIs, public archives, compliant scraping) with schema validation.
      \item Unification and weather integration: align usage to hourly cadence; inner-join hourly weather; derive holidays.
      \item Leakage-safe feature engineering: per-building lag\_1h, lag\_24h, rollmean\_24h; calendar and weather drivers.
      \item Time-aware splitting: chronological train/validation/test that mirror deployment.
      \item Model training and evaluation: strong baselines and boosted trees with diagnostics.
      \item Interpretability and vulnerability profiling: permutation importance, ICE/PDP, temperature elasticity and segments.
      \item Packaging and reporting: reproducible scripts, plots, and programmatic report generation.
      \item Testing and deployment: CLI for batch prediction, monitoring considerations for drift and stability.
    \end{{enumerate}}

    \section{{Problem Definition and Scope}}
    \subsection*{{2.1 Problem Definition}}
    We predict the next-hour normalized usage per building and quantify how sensitive each building's predicted usage is to temperature. Accurate short-horizon forecasts support operations (load balancing, peak management) and analytics (error attribution, risk). Temperature elasticity helps target buildings likely to be more affected by heat waves or cold spells, informing intervention and planning.
    
    \subsection*{{2.2 Scope and Data Acquisition Strategy}}
    We focus on hourly resolution across multiple regions, retaining only hours with weather to avoid bias when analyzing temperature effects. Sources are selected for schema consistency and time semantics. We prefer features that reflect real operational signals (time-of-day, recent usage, weather), and evaluate models with chronological splits that emulate deployment. Below we summarize typical use cases that ground our scope:
    \begin{{itemize}}[leftmargin=*]
      \item \textbf{{Operational planning}}: next-hour forecasts support load balancing and peak management.
      \item \textbf{{Targeted interventions}}: high-sensitivity buildings (elasticity) for demand response and outreach.
      \item \textbf{{Policy evaluation}}: track the distribution of elasticity across time and customer segments.
    \end{{itemize}}



    \section{{Data Acquisition and Dataset Characteristics}}
    \subsection*{{3.1 Data Source and Collection Methodology}}
    We combined two public sources (Ausgrid NSW and London LCL) and standardized them to hourly alignment, merging per-region weather (apparent temperature, precipitation, day/night). We retained only rows with weather present to avoid exogenous imputations. Identifiers are anonymized. The unified dataset serves as the single source of truth for feature engineering and chronological evaluation.
    
    In practice, we first surveyed public archives and APIs for smart-meter or feeder-level usage data across regions. Where APIs were unavailable, we used compliant scraping to collect candidate datasets and validated schemas for usage fields, time resolution, and coverage. Several sources were excluded due to missing usage fields or incompatible granularities. We converged on Ausgrid (NSW, AU) and London LCL as consistent sources. Weather and holiday signals were integrated per region: hourly apparent temperature, precipitation, and day/night flags (temperature normalized), with public holidays derived programmatically (e.g., AU NSW, GB calendars). We kept only rows with weather present to avoid imputing exogenous drivers, preserving integrity for temperature-sensitivity analyses.
    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{ll}}
        \toprule
        Item & Value \\\\ \midrule
        {data_over_latex}
        \\\\ \bottomrule
      \end{{tabular}}
      \caption{{Dataset scope and chronological cutoffs.}}
    \end{{table}}

    \subsection*{{3.2 Technical Challenges and Solutions}}
    - Heterogeneous schemas across sources (CSV vs Parquet) were normalized to a common long format (building-hour rows).
    - Weather alignment: hourly join with strict inner matches; rows without weather removed to preserve interpretability in temperature analyses.
    - Leakage controls: lags/rolling computed per building with shift operations; chronological splits prevent look-ahead.
    - Scale and performance: chunked reads and sampling for EDA; compact figures and tables generated from representative samples.
    
    \subsection*{{3.3 Feature Engineering}}
    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{l p{{10cm}}}}
        \toprule
        Group & Features \\\\ \midrule
        {feat_cat_latex}
        \\\\ \bottomrule
      \end{{tabular}}
      \caption{{Feature groups used in modeling.}}
    \end{{table}}

    \section{{Experimental Design and Splits}}
    We estimate time cutoffs via reservoir sampling of timestamps to approximate 80\%/10\%/10\% train/val/test boundaries.\\
    \textbf{{Train end}}={t_train_end}, \textbf{{Validation end}}={t_val_end}; Test strictly later. Metrics: MAE (primary), RMSE (secondary).

    \section{{Model Development, Training, and Evaluation}}
    \subsection*{{5.1 Feature Selection, Preprocessing, and Initial Analysis}}
    We curated features that reflect operational reality and avoid leakage: per-building lag\_1h and lag\_24h capture persistence and daily cycle; rollmean\_24h summarizes recent regime; calendar features anchor seasonality and hour-level structure; weather features (normalized apparent temperature, precipitation, is\_day) act as exogenous drivers. Preprocessing is minimal by design (trees are scale-robust), focusing effort on correct time alignment and splits.
    \subsection*{{Baselines and Primary Model}}
    \begin{{itemize}}[leftmargin=*]
      \item Naive: $\hat{{y}}(t)=y(t-1)$
      \item RandomForestRegressor (stable baseline)
      \item HistGradientBoostingRegressor (primary)
    \end{{itemize}}
    Training uses large sampled subsets per split to bound memory while preserving time ordering.

    \section{{Key Findings}}
    \begin{{itemize}}[leftmargin=*]
      \item Boosted trees outperform Naive by {imp_mae_val_vs_naive} MAE (val) and {imp_mae_test_vs_naive} MAE (test); RMSE improvements: {imp_rmse_val_vs_naive} (val), {imp_rmse_test_vs_naive} (test).
      \item Temperature, hour-of-day, and recent-usage dynamics rank highly by permutation importance.
      \item Temperature elasticity median = {fmt(med_el)}; segments: Low={seg_low}, Moderate={seg_mod}, High={seg_high}.
    \end{{itemize}}

    \section{{Results}}
    \subsection*{{5.3 Model Performance Evaluation}}
    We evaluate models using MAE (robust to outliers) and RMSE (penalizes large errors) on both validation and hold-out test periods, computed strictly after chronological cutoffs to emulate deployment. Reporting both metrics helps separate median-case fidelity from tail-risk behavior, which is important for operational decision making where occasional spikes can incur high costs.
    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{lcccc}}
        \toprule
        Model & MAE (val) & RMSE (val) & MAE (test) & RMSE (test) \\\\ \midrule
        Naive & {fmt(naive_val_mae)} & {fmt(naive_val_rmse)} & {fmt(naive_test_mae)} & {fmt(naive_test_rmse)} \\\\
        Random Forest & {fmt(rf_val_mae)} & {fmt(rf_val_rmse)} & {fmt(rf_test_mae)} & {fmt(rf_test_rmse)} \\\\
        HistGBR & {fmt(hg_val_mae)} & {fmt(hg_val_rmse)} & {fmt(hg_test_mae)} & {fmt(hg_test_rmse)} \\\\
        \bottomrule
      \end{{tabular}}
      \caption{{Overall accuracy across models (val/test). Primary model: HistGBR.}}
    \end{{table}}

    \subsection*{{Feature Importance (Validation)}}
    Permutation importance highlights {top_feat_names if top_feat_names else "temperature, time-of-day and recent-usage dynamics"} as leading drivers. Higher values indicate stronger contribution to reducing validation MAE; the prominence of temperature and recent usage aligns with expected physical and behavioral effects.
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{{fig_paths["fi"]}}}
      \caption{{Permutation importance (validation) for HistGBR. Dominant features confirm the role of temperature and daily dynamics.}}
    \end{{figure}}

    \noindent Top features (if computed):
    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{lcc}}
        \toprule
        Feature & Importance (mean) & Std \\\\ \midrule
        {top_feat_rows if top_feat_rows else no_table_triple}
        \bottomrule
      \end{{tabular}}
      \caption{{Top features by permutation importance (validation).}}
    \end{{table}}

    \subsection*{{Fit Quality and Error Patterns}}
    The scatter below should concentrate around the diagonal if the model is well calibrated; dispersion at higher usage levels reflects the heavier-tailed regime where short spikes are harder to capture.
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.95\textwidth]{{{fig_paths["scatter"]}}}
      \caption{{Predicted vs True (test sample). Points near the diagonal indicate good calibration; dispersion at high usage reflects spiky demand that remains challenging.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.95\textwidth]{{{fig_paths["resid_hour"]}}}
      \caption{{MAE by hour of day on the test sample. Elevated errors around transition hours (morning/evening) indicate regime changes are hardest to predict.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.95\textwidth]{{{fig_paths["heatmap"]}}}
      \caption{{Error heatmap by hour and month (test sample). Warm bands reveal seasonal-hourly regimes where performance degrades, guiding targeted feature or segment refinements.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.95\textwidth]{{{fig_paths["series"]}}}
      \caption{{Example time series: predicted vs actual for top buildings in sample. The model tracks levels and turning points with small lag except during sharp spikes.}}
    \end{{figure}}

    \subsection*{{Per-Building Test MAE Summary}}
    {per_bld_summary_block}.

    \section{{Model Interpretability with SHAP Analysis}}
    We interpret the model using global importance and response analyses. While SHAP provides additive attributions, for this project we adopt permutation importance (global) and ICE/PDP (local and global response) that convey similar directional insights for tree ensembles. We inspect how predictions change with temperature and time features, and summarize building-level sensitivity via elasticity.
    \subsection*{{Method}}
    We compute per-building temperature elasticity by varying apparent\_temperature\_norm on a fixed grid [0,1] for sampled hours (test period), predicting with the trained model, and fitting a line $dy/dtemp$ across the ICE curve. The building elasticity is the median slope.

    \subsection*{{Segments and Distribution}}
    Segments by quantiles: Low={seg_low}, Moderate={seg_mod}, High={seg_high}. Median elasticity={fmt(med_el)}; Q1={fmt(q25_el)}, Q3={fmt(q75_el)} (scored {n_with_scores}/{n_buildings_total} buildings).
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{{fig_paths["elasticity"]}}}
      \caption{{Distribution of building temperature elasticity (dy/dtemp).}}
    \end{{figure}}

    \subsection*{{Top/Bottom Temperature Sensitivity (Test Period)}}
    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{l c}}
        \toprule
        \multicolumn{{2}}{{c}}{{Top {len(top_high) if top_high else 0} High-Sensitivity Buildings}} \\\\ \midrule
        Building & Elasticity \\\\ \midrule
        {high_rows if high_rows else no_table_double}
        \bottomrule
      \end{{tabular}}
      \caption{{Highest positive elasticity (dy/dtemp).}}
    \end{{table}}

    \begin{{table}}[H]
      \centering
      \begin{{tabular}}{{l c}}
        \toprule
        \multicolumn{{2}}{{c}}{{Top {len(top_low) if top_low else 0} Low-Sensitivity Buildings}} \\\\ \midrule
        Building & Elasticity \\\\ \midrule
        {low_rows if low_rows else no_table_double}
        \bottomrule
      \end{{tabular}}
      \caption{{Lowest elasticity (dy/dtemp).}}
    \end{{table}}

    \section{{Exploratory Data Analysis (EDA)}}
    We first validate the data mechanics (missingness, distributions) and then study relationships. The goal is to understand regimes where the model must perform well and where risk is elevated.
    \subsection*{{4.2 Summary of EDA Findings}}
    - Usage distributions exhibit expected seasonality and diurnal patterns; weekend/holiday behavior differs modestly by region.
    - Error analysis shows the hardest hours are transitions (morning/evening) and hotter temperature bins in some regions.
    - Correlation heatmaps suggest low multicollinearity among engineered signals, a favorable setting for tree models.
    \begin{{itemize}}[leftmargin=*]
      \item Usage vs temperature (normalized) shows expected nonlinearity and heteroscedasticity.
      \item Hour-of-day error patterns indicate highest difficulty during regime transitions (early morning/evening).
      \item Seasonality captured via lag\_24h and monthly effects.
    \end{{itemize}}

    \subsection*{{Extended EDA and Diagnostics (Selected Figures)}}
    The following figures were selected to validate data mechanics and illuminate model-relevant structure. Missingness views ensure inputs are trustworthy; distributional plots reveal heterogeneity across regions and seasons; temporal heatmaps capture diurnal and monthly regimes the model must learn; correlation heatmaps check for multicollinearity that could impair generalization; residual analyses, calibration, and PDP/ICE plots connect model behavior back to physical drivers. Together, these diagnostics justify feature choices and highlight risks that guide improvement.
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/missingness_bar.png}}
      \caption{{Missingness by column (sample).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/missingness_heatmap.png}}
      \caption{{Missingness heatmap (sample x columns).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/usage_hist_by_region.png}}
      \caption{{Usage distribution by region (KDE).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/usage_violin_by_season.png}}
      \caption{{Usage by season (per-building normalized).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/usage_by_hour_box.png}}
      \caption{{Usage by hour of day (boxplot).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/usage_weekend_weekday_violin.png}}
      \caption{{Usage: Weekend vs Weekday.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/usage_holiday_violin.png}}
      \caption{{Usage: Holiday vs Non-Holiday.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/usage_heatmap_hour_dow.png}}
      \caption{{Mean usage heatmap (hour x day\_of\_week).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/usage_heatmap_hour_month.png}}
      \caption{{Mean usage heatmap (hour x month).}}
    \end{{figure}}

    \subsection*{{4.1 Correlation Analysis}}
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/feature_correlation_heatmap.png}}
      \caption{{Feature correlation heatmap (sample).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.8\textwidth]{{../outputs/figures/residual_vs_temp_bin.png}}
      \caption{{Residual vs Temperature (binned MAE).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.8\textwidth]{{../outputs/figures/residual_vs_precip_bin.png}}
      \caption{{Residual vs Precipitation (binned MAE).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.6\textwidth]{{../outputs/figures/calibration_plot.png}}
      \caption{{Calibration: predicted deciles vs true mean.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/pdp_temperature.png}}
      \caption{{PDP: Temperature effect on predicted usage.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/ice_temperature_examples.png}}
      \caption{{ICE: Temperature (10 examples).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/pdp_hour.png}}
      \caption{{PDP: Hour of day.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/pdp_precip.png}}
      \caption{{PDP: Precipitation.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/elasticity_by_region_box.png}}
      \caption{{Elasticity by region (boxplot).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/per_building_mae_hist.png}}
      \caption{{Per-building MAE distribution (test).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/per_building_mae_by_region_box.png}}
      \caption{{Per-building MAE by region (test).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/error_by_season_box.png}}
      \caption{{Error by season (test sample).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/error_by_temp_bin_region.png}}
      \caption{{Error by temperature bin and region. Temperature regimes differ by region; higher bins align with larger errors in hotter periods.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/error_over_time.png}}
      \caption{{Error over time (weekly MAE, test sample). Stable weekly MAE with mild variation suggests good temporal generalization.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.9\textwidth]{{../outputs/figures/worst_buildings_timeseries.png}}
      \caption{{Worst buildings (by MAE): predicted vs actual time series.}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/pca_features_scatter.png}}
      \caption{{PCA of numeric features (sample).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.85\textwidth]{{../outputs/figures/cluster_centers_hourly.png}}
      \caption{{Cluster centers of hourly usage profiles (k=4).}}
    \end{{figure}}

    \begin{{figure}}[H]\centering
      \includegraphics[width=0.75\textwidth]{{../outputs/figures/pairplot_sample.png}}
      \caption{{Pairplot (usage, temp, precip, hour) on a sample.}}
    \end{{figure}}

    \subsection*{{5.2 Model Selection and Training}}
    We compared a naive persistence baseline, Random Forest (variance reduction, stable baseline), and HistGradientBoosting (tabular boosting with strong nonlinear capacity). HistGBR achieved the best validation MAE and generalized well on test. The chart below summarizes validation and test MAE across models.
    \begin{{figure}}[H]\centering
      \includegraphics[width=0.7\textwidth]{{../outputs/figures/model_comparison_bar.png}}
      \caption{{Model comparison (MAE) on validation vs test. HistGBR chosen as primary model.}}
    \end{{figure}}




    \section{{Model Configuration}}
    \subsection*{{RandomForestRegressor}}
    {rf_config}
    \subsection*{{HistGradientBoostingRegressor (primary)}}
    {hgbr_config}



    \section{{Model Testing and Deployment}}
    \subsection*{{7.1 Prediction Script}}
    \subsubsection*{{7.1.1 Purpose and Prerequisites}}
    The prediction CLI produces next-hour predictions per building from a usage file (CSV/Parquet) and optional weather file. It expects hourly timestamps, building identifiers, and usage; with optional apparent temperature and precipitation for higher fidelity.
    \subsubsection*{{7.1.2 Workflow}}
    1) Load the trained model and minimal preprocessing; 2) align/merge weather if provided; 3) compute leakage-safe lags per building for the last hour; 4) predict next-hour usage and write a CSV.
    \subsubsection*{{7.1.3 Command-Line Usage Example}}
    \begin{{verbatim}}
    source .venv/bin/activate && python scripts/predict.py \
      --input data/ausgrid_with_weather_normalized.csv \
      --region NSW \
      --out outputs/tables/predictions_next_hour.csv
    \end{{verbatim}}
    The pipeline is fully scripted and reproducible. Key steps:
    \begin{{itemize}}[leftmargin=*]
      \item \textbf{{Unify data}}: \texttt{{scripts/00\_unify\_datasets.py}} produces a single CSV with region labels.
      \item \textbf{{Build features}}: \texttt{{scripts/01\_build\_features.py}} creates leakage-safe features and the next-hour target.
      \item \textbf{{Train}}: \texttt{{scripts/02\_train\_models.py}} performs chronological splits and trains Naive/RF/HistGBR.
      \item \textbf{{Predict}}: \texttt{{scripts/predict.py}} predicts next-hour usage for each building given a usage file (+ optional weather).
      \item \textbf{{Explain}}: \texttt{{scripts/03\_explain\_segments.py}} computes temperature elasticity and segments.
      \item \textbf{{Plots}}: \texttt{{scripts/04\_make\_plots.py}} + \texttt{{scripts/04a\_extra\_plots.py}} generate $>$30 figures.
      \item \textbf{{Report}}: \texttt{{scripts/05\_render\_report.py}} assembles this LaTeX report.
    \end{{itemize}}
    See the root \texttt{{README.md}} for exact commands and prediction usage examples.

    \subsection*{{7.2 Deployment Considerations}}
    \begin{{itemize}}[leftmargin=*]
      \item \textbf{{Internal validity}}: enforce leakage controls via per-building shifts/rolling; avoid global scalers fit on full data.
      \item \textbf{{External validity}}: distribution shifts (policy changes, technology adoption) may degrade accuracy.
      \item \textbf{{Bias}}: uneven station coverage may bias elasticity; monitor subgroup metrics and confidence intervals.
      \item \textbf{{Privacy}}: anonymized identifiers; avoid re-identification; follow GDPR principles.
    \end{{itemize}}

    \section{{Conclusion}}
    This project delivers an end-to-end, deployment-minded forecasting pipeline for hourly electricity usage that reflects current best practice in the prediction industry: leakage-safe data design, time-aware evaluation, and interpretable, high-performing tree ensembles. By unifying multi-region usage with weather and calendar signals, training with chronological splits, and explaining predictions via global importance and ICE/PDP, we provide both accurate next-hour forecasts and an actionable measure of temperature-driven vulnerability at the building level. The approach has limits: upstream data quality and representativeness can affect both accuracy and elasticity estimates; extreme events and sudden regime shifts remain challenging; and a single global model may underfit niche segments. Future development should explore gradient-boosting libraries with early stopping, uncertainty quantification and monitoring in production, segment-specific models where justified, and richer exogenous drivers (e.g., tariffs or mobility) to improve robustness as the system evolves.

    \section*{{References}}
    \begin{{itemize}}[leftmargin=*]
      \item Pedregosa et al., \emph{{Scikit-learn: Machine Learning in Python}}, JMLR.
      \item Ke et al., \emph{{LightGBM: A Highly Efficient Gradient Boosting Decision Tree}} (future work).
      \item Lundberg \& Lee, \emph{{A Unified Approach to Interpreting Model Predictions}} (future SHAP enhancements).
    \end{{itemize}}


    \end{{document}}
    """)

    # Sanitize LaTeX for Tectonic compatibility:
    # - Convert doubled braces for itemize produced in embedded strings ({{itemize}} -> {itemize})
    # - Replace Unicode punctuation with TeX-safe equivalents
    # - Replace Unicode arrows with math arrows
    paper = (
        paper
        .replace("{{itemize}}", "{itemize}")
        .replace("—", "--")
        .replace("–", "--")
        .replace("‑", "-")
        .replace("’", "'")
        .replace("→", "$\\to$")
        .replace("\\begin{tikzpicture}[", "\\resizebox{0.9\\textwidth}{!}{%\n  \\begin{tikzpicture}[")
        .replace("\\end{tikzpicture}", "\\end{tikzpicture}\n  }%")
    )
    (REPORT_DIR / "paper.tex").write_text(paper)

    readme = dedent("""\
    # Report

    Generated LaTeX source: paper.tex

    Compile (if TeX distribution is installed):
      - latexmk -pdf paper.tex
        or
      - pdflatex paper.tex (run twice for references)

    Figures are referenced from ../outputs/figures/*.png.
    Metrics/tables sourced from ../outputs/metrics and ../outputs/tables.

    If you do not have LaTeX installed locally, you can upload `paper.tex` and the `outputs/figures` directory to Overleaf and compile there.
    """)
    (REPORT_DIR / "README.md").write_text(readme)

    print(f"Wrote {REPORT_DIR / 'paper.tex'}")
    print(f"Wrote {REPORT_DIR / 'README.md'}")

if __name__ == "__main__":
    main()
